{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60826c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qcnace/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dataloader\n",
    "CBOW_N_WORDS=4\n",
    "SKIPGRAM_N_WORDS=4\n",
    "MIN_WORD_FREQUENCY=50\n",
    "MAX_SEQUENCE_LENGTH=256\n",
    "EMBED_DIMENSION=300\n",
    "EMBED_MAX_NORM=1\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed587833",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-07 15:32:24,075 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from wikitext. Please make sure that you can trust the external codes.\n",
      "2025-05-07 15:32:32,994 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from wikitext. Please make sure that you can trust the external codes.\n",
      "2025-05-07 15:32:35,239 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from wikitext. Please make sure that you can trust the external codes.\n",
      "2025-05-07 15:32:38,090 - modelscope - WARNING - Use trust_remote_code=True. Will invoke codes from wikitext. Please make sure that you can trust the external codes.\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import to_map_style_dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from modelscope.msdatasets import MsDataset\n",
    "train_data_iter = to_map_style_dataset(MsDataset.load('wikitext', subset_name='wikitext-2-v1', split='train'))\n",
    "valid_data_iter = to_map_style_dataset(MsDataset.load('wikitext', subset_name='wikitext-2-v1', split='validation'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45fb81f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_iter=[i['text'] for i in train_data_iter if i['text'] != '' ]\n",
    "valid_data_iter=[i['text'] for i in valid_data_iter if i['text'] != '' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "035e8f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' = Homarus gammarus = \\n',\n",
       " ' Homarus gammarus , known as the European lobster or common lobster , is a species of <unk> lobster from the eastern Atlantic Ocean , Mediterranean Sea and parts of the Black Sea . It is closely related to the American lobster , H. americanus . It may grow to a length of 60 cm ( 24 in ) and a mass of 6 kilograms ( 13 lb ) , and bears a conspicuous pair of claws . In life , the lobsters are blue , only becoming \" lobster red \" on cooking . Mating occurs in the summer , producing eggs which are carried by the females for up to a year before hatching into <unk> larvae . Homarus gammarus is a highly esteemed food , and is widely caught using lobster pots , mostly around the British Isles . \\n',\n",
       " ' = = Description = = \\n',\n",
       " ' Homarus gammarus is a large <unk> , with a body length up to 60 centimetres ( 24 in ) and weighing up to 5 – 6 kilograms ( 11 – 13 lb ) , although the lobsters caught in lobster pots are usually 23 – 38 cm ( 9 – 15 in ) long and weigh 0 @.@ 7 – 2 @.@ 2 kg ( 1 @.@ 5 – 4 @.@ 9 lb ) . Like other crustaceans , lobsters have a hard <unk> which they must shed in order to grow , in a process called <unk> ( <unk> ) . This may occur several times a year for young lobsters , but decreases to once every 1 – 2 years for larger animals . \\n',\n",
       " ' The first pair of <unk> is armed with a large , asymmetrical pair of claws . The larger one is the \" <unk> \" , and has rounded <unk> used for crushing prey ; the other is the \" cutter \" , which has sharp inner edges , and is used for holding or tearing the prey . Usually , the left claw is the <unk> , and the right is the cutter . \\n',\n",
       " ' The <unk> is generally blue above , with spots that <unk> , and yellow below . The red colour associated with lobsters only appears after cooking . This occurs because , in life , the red pigment <unk> is bound to a protein complex , but the complex is broken up by the heat of cooking , releasing the red pigment . \\n',\n",
       " ' The closest relative of H. gammarus is the American lobster , Homarus americanus . The two species are very similar , and can be crossed artificially , although hybrids are unlikely to occur in the wild since their ranges do not overlap . The two species can be distinguished by a number of characteristics : \\n',\n",
       " ' The <unk> of H. americanus bears one or more spines on the underside , which are lacking in H. gammarus . \\n',\n",
       " ' The spines on the claws of H. americanus are red or red @-@ tipped , while those of H. gammarus are white or white @-@ tipped . \\n',\n",
       " ' The underside of the claw of H. americanus is orange or red , while that of H. gammarus is creamy white or very pale red . \\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data_iter[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1ea49b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab_from_iterator(\n",
    "        map(tokenizer, train_data_iter+valid_data_iter),\n",
    "        specials=[\"<unk>\"],\n",
    "        min_freq=MIN_WORD_FREQUENCY,\n",
    "    )\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fe2ccba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4393"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16030454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def collate_cbow(batch,text_pipeline):\n",
    "    batch_input, batch_output = [], []\n",
    "    for text in batch:\n",
    "        text_tokens_ids=text_pipeline(text)\n",
    "        if len(text_tokens_ids) < CBOW_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "        for idx in range(len(text_tokens_ids) - CBOW_N_WORDS * 2):\n",
    "            token_id_sequence = text_tokens_ids[idx : (idx + CBOW_N_WORDS * 2 + 1)]\n",
    "            output = token_id_sequence.pop(CBOW_N_WORDS)\n",
    "            input_ = token_id_sequence\n",
    "            batch_input.append(input_)\n",
    "            batch_output.append(output)\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "    return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3babac42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_skipgram(batch, text_pipeline):\n",
    "    \"\"\"\n",
    "    Collate_fn for Skip-Gram model to be used with Dataloader.\n",
    "    `batch` is expected to be list of text paragrahs.\n",
    "    \n",
    "    Context is represented as N=SKIPGRAM_N_WORDS past words \n",
    "    and N=SKIPGRAM_N_WORDS future words.\n",
    "    \n",
    "    Long paragraphs will be truncated to contain\n",
    "    no more that MAX_SEQUENCE_LENGTH tokens.\n",
    "    \n",
    "    Each element in `batch_input` is a middle word.\n",
    "    Each element in `batch_output` is a context word.\n",
    "    \"\"\"\n",
    "    batch_input, batch_output = [], []\n",
    "    for text in batch:\n",
    "        text_tokens_ids = text_pipeline(text)\n",
    "\n",
    "        if len(text_tokens_ids) < SKIPGRAM_N_WORDS * 2 + 1:\n",
    "            continue\n",
    "\n",
    "        if MAX_SEQUENCE_LENGTH:\n",
    "            text_tokens_ids = text_tokens_ids[:MAX_SEQUENCE_LENGTH]\n",
    "\n",
    "        for idx in range(len(text_tokens_ids) - SKIPGRAM_N_WORDS * 2):\n",
    "            token_id_sequence = text_tokens_ids[idx : (idx + SKIPGRAM_N_WORDS * 2 + 1)]\n",
    "            input_ = token_id_sequence.pop(SKIPGRAM_N_WORDS)\n",
    "            outputs = token_id_sequence\n",
    "\n",
    "            for output in outputs:\n",
    "                batch_input.append(input_)\n",
    "                batch_output.append(output)\n",
    "\n",
    "    batch_input = torch.tensor(batch_input, dtype=torch.long)\n",
    "    batch_output = torch.tensor(batch_output, dtype=torch.long)\n",
    "    return batch_input, batch_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514a87ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: vocab(tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93ed120b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4393\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(\n",
    "    train_data_iter,\n",
    "    batch_size=96,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_skipgram, text_pipeline=text_pipeline)\n",
    "    )\n",
    "\n",
    "val_dataloader= DataLoader(\n",
    "    valid_data_iter,\n",
    "    batch_size=96,\n",
    "    shuffle=True,\n",
    "    collate_fn=partial(collate_skipgram, text_pipeline=text_pipeline)\n",
    ")\n",
    "vocab_size = len(vocab.get_stoi())\n",
    "print(f\"Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52de4529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa75a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self,vocab_size:int):\n",
    "        super(CBOW,self).__init__()\n",
    "        self.embeddings=nn.Embedding(num_embeddings=vocab_size,embedding_dim=EMBED_DIMENSION,max_norm=EMBED_MAX_NORM)\n",
    "        self.linear=nn.Linear(in_features=EMBED_DIMENSION,out_features=vocab_size)\n",
    "    def forward(self,inputs_):\n",
    "        x=self.embeddings(inputs_)\n",
    "        x=x.mean(dim=1)\n",
    "        x=self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef43f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size: int):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(\n",
    "            num_embeddings=vocab_size,\n",
    "            embedding_dim=EMBED_DIMENSION,\n",
    "            max_norm=EMBED_MAX_NORM,\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            in_features=EMBED_DIMENSION,\n",
    "            out_features=vocab_size,\n",
    "        )\n",
    "    def forward(self, inputs_):\n",
    "        x = self.embeddings(inputs_)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99c7d4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SkipGram(vocab_size=vocab_size)\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.025)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afb6991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"Main class for model training\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        epochs,\n",
    "        train_dataloader,\n",
    "        train_steps,\n",
    "        val_dataloader,\n",
    "        val_steps,\n",
    "        checkpoint_frequency,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        device,\n",
    "        model_dir,\n",
    "        model_name,\n",
    "    ):  \n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.train_steps = train_steps\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.val_steps = val_steps\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.checkpoint_frequency = checkpoint_frequency\n",
    "        self.device = device\n",
    "        self.model_dir = model_dir\n",
    "        self.model_name = model_name\n",
    "\n",
    "        self.loss = {\"train\": [], \"val\": []}\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            lr_lambda = lambda epoch: max(0.001, (5 - epoch) / 5)  # 最低学习率=0.001\n",
    "            lr_scheduler = LambdaLR(optimizer, lr_lambda, verbose=True)\n",
    "            print(\"Current LR:\", optimizer.param_groups[0]['lr'])\n",
    "            self._train_epoch()\n",
    "            self._validate_epoch()\n",
    "            print(\n",
    "                \"Epoch: {}/{}, Train Loss={:.5f}, Val Loss={:.5f}\".format(\n",
    "                    epoch + 1,\n",
    "                    self.epochs,\n",
    "                    self.loss[\"train\"][-1],\n",
    "                    self.loss[\"val\"][-1],\n",
    "                )\n",
    "            )\n",
    "\n",
    "            lr_scheduler.step()\n",
    "\n",
    "            # if self.checkpoint_frequency:\n",
    "            #     self._save_checkpoint(epoch)\n",
    "        self.save_model()\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        self.model.train()\n",
    "        running_loss = []\n",
    "\n",
    "        for i, batch_data in enumerate(self.train_dataloader, 1):\n",
    "            inputs = batch_data[0].to(self.device)\n",
    "            labels = batch_data[1].to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            outputs = self.model(inputs)\n",
    "            \n",
    "            loss = self.criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            running_loss.append(loss.item())\n",
    "\n",
    "            if i == self.train_steps:\n",
    "                break\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss[\"train\"].append(epoch_loss)\n",
    "\n",
    "    def _validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        running_loss = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch_data in enumerate(self.val_dataloader, 1):\n",
    "                inputs = batch_data[0].to(self.device)\n",
    "                labels = batch_data[1].to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "                running_loss.append(loss.item())\n",
    "\n",
    "                if i == self.val_steps:\n",
    "                    break\n",
    "\n",
    "        epoch_loss = np.mean(running_loss)\n",
    "        self.loss[\"val\"].append(epoch_loss)\n",
    "\n",
    "    def _save_checkpoint(self, epoch):\n",
    "        \"\"\"Save model checkpoint to `self.model_dir` directory\"\"\"\n",
    "        epoch_num = epoch + 1\n",
    "        if epoch_num % self.checkpoint_frequency == 0:\n",
    "            model_path = \"checkpoint_{}.pt\".format(str(epoch_num).zfill(3))\n",
    "            model_path = os.path.join(self.model_dir, model_path)\n",
    "            torch.save(self.model, model_path)\n",
    "\n",
    "    def save_model(self):\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        }, './model.pt')\n",
    "\n",
    "    def save_loss(self):\n",
    "        \"\"\"Save train/val loss as json file to `self.model_dir` directory\"\"\"\n",
    "        loss_path = os.path.join(self.model_dir, \"loss.json\")\n",
    "        with open(loss_path, \"w\") as fp:\n",
    "            json.dump(self.loss, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93947d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 1/20, Train Loss=5.80971, Val Loss=5.64698\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 2/20, Train Loss=5.58196, Val Loss=5.63213\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 3/20, Train Loss=5.56033, Val Loss=5.61181\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 4/20, Train Loss=5.54211, Val Loss=5.59848\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 5/20, Train Loss=5.53832, Val Loss=5.59105\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 6/20, Train Loss=5.52926, Val Loss=5.57200\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 7/20, Train Loss=5.50915, Val Loss=5.56531\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 8/20, Train Loss=5.50037, Val Loss=5.56493\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 9/20, Train Loss=5.49587, Val Loss=5.55943\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 10/20, Train Loss=5.48456, Val Loss=5.55226\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 11/20, Train Loss=5.48636, Val Loss=5.55615\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 12/20, Train Loss=5.47132, Val Loss=5.54445\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 13/20, Train Loss=5.47317, Val Loss=5.53797\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 14/20, Train Loss=5.47791, Val Loss=5.54996\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 15/20, Train Loss=5.47050, Val Loss=5.54256\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 16/20, Train Loss=5.47601, Val Loss=5.54046\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 17/20, Train Loss=5.47868, Val Loss=5.54329\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 18/20, Train Loss=5.46284, Val Loss=5.53894\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 19/20, Train Loss=5.47250, Val Loss=5.54549\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Adjusting learning rate of group 0 to 2.5000e-02.\n",
      "Current LR: 0.025\n",
      "Epoch: 20/20, Train Loss=5.46788, Val Loss=5.54473\n",
      "Adjusting learning rate of group 0 to 2.0000e-02.\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    epochs=20,\n",
    "    train_dataloader=train_dataloader,\n",
    "    train_steps=100,\n",
    "    val_dataloader=val_dataloader,\n",
    "    val_steps=100,\n",
    "    checkpoint_frequency=20,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    model_dir='.',\n",
    "    model_name='c',\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "trainer.save_model()\n",
    "trainer.save_loss()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a597a41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictions: [('outbreak', 0.9938902854919434), ('pitcher', 0.9838512539863586), ('cong', 0.9765623211860657), ('i', 0.9908341765403748), ('pitcher', 0.9838512539863586), ('outbreak', 0.9938902854919434), ('i', 0.9926117062568665), ('communication', 0.9883968234062195), ('95', 0.986205518245697), ('occasion', 0.9820840954780579), ('knots', 0.9804263114929199), ('war', 0.9809697866439819)]\n"
     ]
    }
   ],
   "source": [
    "#推理阶段\n",
    "model.eval()\n",
    "model.to(device)\n",
    "def preprocess_text(text, vocab, context_window=1):\n",
    "    token_indices =  text_pipeline(text)\n",
    "    inputs, targets = [], []\n",
    "    for i in range(context_window, len(token_indices) - context_window):\n",
    "        context = token_indices[i-context_window : i] + token_indices[i+1 : i+context_window+1]\n",
    "        inputs.append(context)\n",
    "    return torch.tensor(inputs,dtype=torch.long)  \n",
    "def predict_word(model, context_tensor, vocab, top_k=1):\n",
    "    with torch.no_grad():\n",
    "        logits = model(context_tensor)  # [num_samples, vocab_size]\n",
    "        probs = torch.softmax(logits, dim=1)  # [num_samples, vocab_size]\n",
    "    \n",
    "    # 对每个样本分别取 top_k\n",
    "    top_probs, top_indices = torch.topk(probs, k=top_k)  # [num_samples, k]\n",
    "    \n",
    "    idx_to_word = vocab.get_itos()\n",
    "    predictions = []\n",
    "    for i in range(top_probs.shape[0]):  # 遍历每个样本\n",
    "        sample_preds = [\n",
    "            (idx_to_word[idx.item()], prob.item()) \n",
    "            for idx, prob in zip(top_indices[i], top_probs[i])\n",
    "        ]\n",
    "        predictions.extend(sample_preds)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# 使用示例\n",
    "text = \"I wonder who am i in this world\"\n",
    "context_tensor = preprocess_text(text, vocab) \n",
    "context_tensor = context_tensor.to(device)\n",
    "predictions = predict_word(model, context_tensor, vocab)\n",
    "print(\"Top predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
